{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-09-17T16:19:35.757390Z","iopub.status.busy":"2023-09-17T16:19:35.757031Z","iopub.status.idle":"2023-09-17T16:19:42.047616Z","shell.execute_reply":"2023-09-17T16:19:42.046584Z","shell.execute_reply.started":"2023-09-17T16:19:35.757356Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}],"source":["from datasets import load_dataset\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","from sklearn.metrics import accuracy_score\n","from transformers import ConvNextFeatureExtractor\n","from transformers import AutoModelForImageClassification"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-09-17T16:19:42.050056Z","iopub.status.busy":"2023-09-17T16:19:42.049136Z","iopub.status.idle":"2023-09-17T16:19:42.091286Z","shell.execute_reply":"2023-09-17T16:19:42.090381Z","shell.execute_reply.started":"2023-09-17T16:19:42.050020Z"},"trusted":true},"outputs":[],"source":["transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) \n","])\n","train_set = datasets.ImageFolder(root = \"/kaggle/input/chest-xray-pneumonia/chest_xray/train/\",\n","                                 transform =transform)\n","\n","test_set = datasets.ImageFolder(root=\"/kaggle/input/chest-xray-pneumonia/chest_xray/test/\"\n","                               ,transform= transform)\n","\n","train_loader = DataLoader(train_set,batch_size=32,shuffle=True)\n","test_loader = DataLoader(test_set,batch_size=32,shuffle=True)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-09-17T16:19:42.094835Z","iopub.status.busy":"2023-09-17T16:19:42.094562Z","iopub.status.idle":"2023-09-17T16:19:42.245335Z","shell.execute_reply":"2023-09-17T16:19:42.244390Z","shell.execute_reply.started":"2023-09-17T16:19:42.094811Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/models/convnext/feature_extraction_convnext.py:28: FutureWarning: The class ConvNextFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ConvNextImageProcessor instead.\n","  warnings.warn(\n"]}],"source":["feature_extractor = ConvNextFeatureExtractor.from_pretrained(\"facebook/convnext-tiny-224\")"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-09-17T16:19:46.949106Z","iopub.status.busy":"2023-09-17T16:19:46.948408Z","iopub.status.idle":"2023-09-17T16:19:46.955466Z","shell.execute_reply":"2023-09-17T16:19:46.954436Z","shell.execute_reply.started":"2023-09-17T16:19:46.949071Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["ConvNextFeatureExtractor {\n","  \"crop_pct\": 0.875,\n","  \"do_normalize\": true,\n","  \"do_rescale\": true,\n","  \"do_resize\": true,\n","  \"feature_extractor_type\": \"ConvNextFeatureExtractor\",\n","  \"image_mean\": [\n","    0.485,\n","    0.456,\n","    0.406\n","  ],\n","  \"image_processor_type\": \"ConvNextFeatureExtractor\",\n","  \"image_std\": [\n","    0.229,\n","    0.224,\n","    0.225\n","  ],\n","  \"resample\": 3,\n","  \"rescale_factor\": 0.00392156862745098,\n","  \"size\": {\n","    \"shortest_edge\": 224\n","  }\n","}\n","\n"]}],"source":["print(feature_extractor)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-09-17T16:19:49.063111Z","iopub.status.busy":"2023-09-17T16:19:49.062747Z","iopub.status.idle":"2023-09-17T16:19:50.054824Z","shell.execute_reply":"2023-09-17T16:19:50.053839Z","shell.execute_reply.started":"2023-09-17T16:19:49.063084Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of ConvNextForImageClassification were not initialized from the model checkpoint at facebook/convnext-tiny-224 and are newly initialized because the shapes did not match:\n","- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n","- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["model = AutoModelForImageClassification.from_pretrained(\"facebook/convnext-tiny-224\",\n","                                                        num_labels=2,\n","                                                        ignore_mismatched_sizes=True)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-09-17T16:19:52.205666Z","iopub.status.busy":"2023-09-17T16:19:52.205277Z","iopub.status.idle":"2023-09-17T16:19:54.624177Z","shell.execute_reply":"2023-09-17T16:19:54.623211Z","shell.execute_reply.started":"2023-09-17T16:19:52.205637Z"},"trusted":true},"outputs":[{"data":{"text/plain":["ConvNextForImageClassification(\n","  (convnext): ConvNextModel(\n","    (embeddings): ConvNextEmbeddings(\n","      (patch_embeddings): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n","      (layernorm): ConvNextLayerNorm()\n","    )\n","    (encoder): ConvNextEncoder(\n","      (stages): ModuleList(\n","        (0): ConvNextStage(\n","          (downsampling_layer): Identity()\n","          (layers): Sequential(\n","            (0): ConvNextLayer(\n","              (dwconv): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n","              (layernorm): ConvNextLayerNorm()\n","              (pwconv1): Linear(in_features=96, out_features=384, bias=True)\n","              (act): GELUActivation()\n","              (pwconv2): Linear(in_features=384, out_features=96, bias=True)\n","              (drop_path): Identity()\n","            )\n","            (1): ConvNextLayer(\n","              (dwconv): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n","              (layernorm): ConvNextLayerNorm()\n","              (pwconv1): Linear(in_features=96, out_features=384, bias=True)\n","              (act): GELUActivation()\n","              (pwconv2): Linear(in_features=384, out_features=96, bias=True)\n","              (drop_path): Identity()\n","            )\n","            (2): ConvNextLayer(\n","              (dwconv): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n","              (layernorm): ConvNextLayerNorm()\n","              (pwconv1): Linear(in_features=96, out_features=384, bias=True)\n","              (act): GELUActivation()\n","              (pwconv2): Linear(in_features=384, out_features=96, bias=True)\n","              (drop_path): Identity()\n","            )\n","          )\n","        )\n","        (1): ConvNextStage(\n","          (downsampling_layer): Sequential(\n","            (0): ConvNextLayerNorm()\n","            (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n","          )\n","          (layers): Sequential(\n","            (0): ConvNextLayer(\n","              (dwconv): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n","              (layernorm): ConvNextLayerNorm()\n","              (pwconv1): Linear(in_features=192, out_features=768, bias=True)\n","              (act): GELUActivation()\n","              (pwconv2): Linear(in_features=768, out_features=192, bias=True)\n","              (drop_path): Identity()\n","            )\n","            (1): ConvNextLayer(\n","              (dwconv): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n","              (layernorm): ConvNextLayerNorm()\n","              (pwconv1): Linear(in_features=192, out_features=768, bias=True)\n","              (act): GELUActivation()\n","              (pwconv2): Linear(in_features=768, out_features=192, bias=True)\n","              (drop_path): Identity()\n","            )\n","            (2): ConvNextLayer(\n","              (dwconv): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n","              (layernorm): ConvNextLayerNorm()\n","              (pwconv1): Linear(in_features=192, out_features=768, bias=True)\n","              (act): GELUActivation()\n","              (pwconv2): Linear(in_features=768, out_features=192, bias=True)\n","              (drop_path): Identity()\n","            )\n","          )\n","        )\n","        (2): ConvNextStage(\n","          (downsampling_layer): Sequential(\n","            (0): ConvNextLayerNorm()\n","            (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n","          )\n","          (layers): Sequential(\n","            (0): ConvNextLayer(\n","              (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n","              (layernorm): ConvNextLayerNorm()\n","              (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n","              (act): GELUActivation()\n","              (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n","              (drop_path): Identity()\n","            )\n","            (1): ConvNextLayer(\n","              (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n","              (layernorm): ConvNextLayerNorm()\n","              (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n","              (act): GELUActivation()\n","              (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n","              (drop_path): Identity()\n","            )\n","            (2): ConvNextLayer(\n","              (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n","              (layernorm): ConvNextLayerNorm()\n","              (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n","              (act): GELUActivation()\n","              (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n","              (drop_path): Identity()\n","            )\n","            (3): ConvNextLayer(\n","              (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n","              (layernorm): ConvNextLayerNorm()\n","              (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n","              (act): GELUActivation()\n","              (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n","              (drop_path): Identity()\n","            )\n","            (4): ConvNextLayer(\n","              (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n","              (layernorm): ConvNextLayerNorm()\n","              (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n","              (act): GELUActivation()\n","              (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n","              (drop_path): Identity()\n","            )\n","            (5): ConvNextLayer(\n","              (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n","              (layernorm): ConvNextLayerNorm()\n","              (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n","              (act): GELUActivation()\n","              (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n","              (drop_path): Identity()\n","            )\n","            (6): ConvNextLayer(\n","              (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n","              (layernorm): ConvNextLayerNorm()\n","              (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n","              (act): GELUActivation()\n","              (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n","              (drop_path): Identity()\n","            )\n","            (7): ConvNextLayer(\n","              (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n","              (layernorm): ConvNextLayerNorm()\n","              (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n","              (act): GELUActivation()\n","              (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n","              (drop_path): Identity()\n","            )\n","            (8): ConvNextLayer(\n","              (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n","              (layernorm): ConvNextLayerNorm()\n","              (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n","              (act): GELUActivation()\n","              (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n","              (drop_path): Identity()\n","            )\n","          )\n","        )\n","        (3): ConvNextStage(\n","          (downsampling_layer): Sequential(\n","            (0): ConvNextLayerNorm()\n","            (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n","          )\n","          (layers): Sequential(\n","            (0): ConvNextLayer(\n","              (dwconv): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n","              (layernorm): ConvNextLayerNorm()\n","              (pwconv1): Linear(in_features=768, out_features=3072, bias=True)\n","              (act): GELUActivation()\n","              (pwconv2): Linear(in_features=3072, out_features=768, bias=True)\n","              (drop_path): Identity()\n","            )\n","            (1): ConvNextLayer(\n","              (dwconv): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n","              (layernorm): ConvNextLayerNorm()\n","              (pwconv1): Linear(in_features=768, out_features=3072, bias=True)\n","              (act): GELUActivation()\n","              (pwconv2): Linear(in_features=3072, out_features=768, bias=True)\n","              (drop_path): Identity()\n","            )\n","            (2): ConvNextLayer(\n","              (dwconv): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n","              (layernorm): ConvNextLayerNorm()\n","              (pwconv1): Linear(in_features=768, out_features=3072, bias=True)\n","              (act): GELUActivation()\n","              (pwconv2): Linear(in_features=3072, out_features=768, bias=True)\n","              (drop_path): Identity()\n","            )\n","          )\n","        )\n","      )\n","    )\n","    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","  )\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model.to(device)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-09-17T16:20:02.059211Z","iopub.status.busy":"2023-09-17T16:20:02.058798Z","iopub.status.idle":"2023-09-17T16:32:26.662302Z","shell.execute_reply":"2023-09-17T16:32:26.661236Z","shell.execute_reply.started":"2023-09-17T16:20:02.059178Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/5] Loss: 0.1623, Accuracy: 0.8301\n","Epoch [2/5] Loss: 0.0522, Accuracy: 0.7500\n","Epoch [3/5] Loss: 0.0378, Accuracy: 0.6843\n","Epoch [4/5] Loss: 0.0321, Accuracy: 0.8510\n","Epoch [5/5] Loss: 0.0326, Accuracy: 0.7436\n","Training complete.\n"]}],"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.AdamW(model.parameters(), lr=0.001)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","num_epochs = 5\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","\n","    for inputs, labels in train_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = model(inputs)\n","        loss = criterion(outputs.logits, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","    model.eval()\n","    all_preds = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for inputs, labels in test_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            outputs = model(inputs)\n","            _, preds = torch.max(outputs.logits, 1)\n","\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    accuracy = accuracy_score(all_labels, all_preds)\n","    print(f\"Epoch [{epoch + 1}/{num_epochs}] Loss: {running_loss / len(train_loader):.4f}, Accuracy: {accuracy:.4f}\")\n","\n","print(\"Training complete.\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.6 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat":4,"nbformat_minor":4}
